{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def prvar(__x):\n",
    "    print(traceback.extract_stack(limit=2)[0][3][6:][:-1],\"=\",__x)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## For TEST\n",
    "\n",
    "# folder_name= 'data/sides'\n",
    "# course_name= '2020-2021_q_all'\n",
    "# train_file= 'train_data.csv'\n",
    "# test_file= 'test_data.csv'\n",
    "# kc_col_name= 'specialty'\n",
    "# min_interactions_per_user= 1\n",
    "# remove_nan_skills= True\n",
    "# verbose= True\n",
    "# drop_duplicates= True\n",
    "# remove_nan_answer_type= True\n",
    "# min_answer_per_question= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_sides(folder_name, course_name, train_file, test_file, kc_col_name, min_interactions_per_user, min_answer_per_question, remove_nan_skills, remove_nan_answer_type, verbose, drop_duplicates=True):\n",
    "    '''\n",
    "    Reading input files\n",
    "    Drop rows for which topic is not determined\n",
    "    Return the pre-processed file and Q-matrix (for question-specialty matches).\n",
    "    \n",
    "    Arguments:\n",
    "    folder_name -- path to the folder containig kdd files (algebra05, bridge_algebra06)\n",
    "    course_name -- name of the course for which pre_processing is executed\n",
    "    train_file -- original train_file provided by KDD cup organizers\n",
    "    test_file -- original test_file provided by KDD cup organizers\n",
    "    kc_col_name -- Skills id column\n",
    "    min_interactions_per_user -- minimum number of interactions per student\n",
    "    drop_duplicates -- if True, drop duplicates from dataset\n",
    "    \n",
    "    Outputs:\n",
    "    data -- preprocessed dataset (pandas DataFrame)\n",
    "    Q_mat -- corresponding q-matrix (item-skill relationships sparse array)\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(folder_name):\n",
    "        print(\"The provided path for the data is invalid and the function will not be executed.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    \n",
    "        \n",
    "    # reading csv file containing information about students' practice (attempt) history\n",
    "    # from the train and test file provided by KDD organizer and then concatante them.\n",
    "    train_file_path = folder_name  +'/'+  course_name  +'/'+ train_file\n",
    "    df_train = pd.read_csv(train_file_path).rename(columns={\n",
    "        'student': 'student',\n",
    "        'question': 'question',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'date_time': 'timestamp',\n",
    "        'result': 'correct',\n",
    "        'n_answer_option': 'n_options',\n",
    "        'answer_option': 'answer_type',\n",
    "        'type': 'quest_type',\n",
    "    })[['student', 'question','correct', 'timestamp', 'kc_id','n_options','answer_type','quest_type']]\n",
    "    if verbose:\n",
    "        initial_shape = df_train.shape[0]\n",
    "        print(\"Opened SIDES train data. Output: {} samples.\".format(initial_shape))\n",
    "        # also save the verbose information in a verbose text file\n",
    "        if not os.path.isdir(folder_name+'/'+ course_name+\"/processed\"):\n",
    "            os.makedirs(folder_name+'/'+ course_name+\"/processed\")\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"w\") as text_file:\n",
    "            text_file.write(\"Opened SIDES train data. Output: {} samples.\\n\".format(initial_shape))\n",
    "        \n",
    "    test_file_path = folder_name  +'/'+  course_name  +'/'+ test_file\n",
    "    df_test = pd.read_csv(test_file_path).rename(columns={\n",
    "        'student': 'student',\n",
    "        'question': 'question',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'date_time': 'timestamp',\n",
    "        'result': 'correct',\n",
    "        'n_answer_option': 'n_options',\n",
    "        'answer_option': 'answer_type',\n",
    "        'type': 'quest_type',\n",
    "    })[['student', 'question', 'correct', 'timestamp', 'kc_id','n_options','answer_type','quest_type']]\n",
    "    if verbose:\n",
    "        initial_shape = df_test.shape[0]\n",
    "        print(\"Opened SIDES test data. Output: {} samples.\".format(initial_shape))\n",
    "        # also save the verbose information in a verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"Opened SIDES test data. Output: {} samples.\\n\".format(initial_shape))\n",
    "        \n",
    "    #merge df_tarin & df_test    \n",
    "    df_train['group'] = 0\n",
    "    df_test['group'] = 1\n",
    "    frames = [df_train, df_test]\n",
    "    data = pd.concat(frames)\n",
    "    del df_train\n",
    "    del df_test\n",
    "    \n",
    "    if verbose:\n",
    "        initial_shape = data.shape[0]\n",
    "        print(\"Merged train and test data. Output: {} samples.\".format(initial_shape))\n",
    "        print(\"Number of unique students: {}.\".format(data.student.nunique()))\n",
    "        print(\"Number of unique questions: {}.\".format(data.question.nunique()))\n",
    "        print(\"Number of rows: {}.\".format(data.shape[0]))\n",
    "        \n",
    "        # also save the verbose information in a verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"Merged train and test data. Output: {} samples.\\n\".format(initial_shape))\n",
    "            text_file.write(\"Number of unique students: {}.\\n\".format(data.student.nunique()))\n",
    "            text_file.write(\"Number of unique questions: {}.\\n\".format(data.question.nunique()))\n",
    "            text_file.write(\"Number of rows: {}.\\n\".format(data.shape[0]))\n",
    "    \n",
    "    \n",
    "    # 1- number of unique questions and number of lines in each quest_type\n",
    "    question_types_number=data.groupby('quest_type').question.nunique()\n",
    "    print('1- number of unique questions in each quest_type',question_types_number)\n",
    "    # write the same information in the verbose text file\n",
    "    with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "        text_file.write('1- number of unique questions in each quest_type\\n')\n",
    "        text_file.write(str(question_types_number))\n",
    "        text_file.write('\\n')\n",
    "        \n",
    "\n",
    "    # 2- Remove potential duplicates\n",
    "    #data= df_train.copy()\n",
    "    initial_shape = data.shape[0]\n",
    "    data = data[~data.duplicated()]\n",
    "    if verbose:\n",
    "        # print how many duplicates have been found out of the total number of samples\n",
    "        print('2- Remove Potential duplicates')\n",
    "        print(\"Removed {} duplicated samples.\".format(initial_shape-data.shape[0]))\n",
    "        print (\"out of {} samples\".format(initial_shape))\n",
    "        print(\"percentage of duplicates: {}%\".format((initial_shape-data.shape[0])/initial_shape*100))\n",
    "        # write the same information in the verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write('2- Remove Potential duplicates\\n')\n",
    "            text_file.write(\"Removed {} duplicated samples.\\n\".format(initial_shape-data.shape[0]))\n",
    "            text_file.write (\"out of {} samples.\\n\".format(initial_shape))\n",
    "            text_file.write(\"percentage of duplicates: {}%.\\n\".format((initial_shape-data.shape[0])/initial_shape*100))\n",
    "    \n",
    "    \n",
    "    # 3- remove questions without specialty if remove_questions_without_spec is True\n",
    "    initial_n_unique_questions = data.question.nunique()\n",
    "    initial_shape = data.shape[0]\n",
    "    if remove_nan_skills:\n",
    "        data = data[~data[\"kc_id\"].isnull()]\n",
    "        if verbose:\n",
    "            print('3- Remove questions without specialty')\n",
    "            print(\"Removed {} samples with NaN skills.\".format(initial_shape-data.shape[0]))\n",
    "            print(\"Percentage of removed lines: {}%\".format((initial_shape-data.shape[0])/initial_shape*100))\n",
    "            print(\"Percentage of removed unique questions: {}%\".format((initial_n_unique_questions-data.question.nunique())/initial_n_unique_questions*100))\n",
    "            # write the same information in the verbose text file\n",
    "            with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "                text_file.write('3- Remove questions without specialty\\n')\n",
    "                text_file.write(\"Removed {} samples with NaN skills.\\n\".format(initial_shape-data.shape[0]))\n",
    "                text_file.write(\"Percentage of removed lines: {}%.\\n\".format((initial_shape-data.shape[0])/initial_shape*100))\n",
    "                text_file.write(\"Percentage of removed unique questions: {}%.\\n\".format((initial_n_unique_questions-data.question.nunique())/initial_n_unique_questions*100))\n",
    "    else:\n",
    "        data.loc[data[\"kc_id\"].isnull(), \"kc_id\"] = 'NaN'\n",
    "        print(\"3- Questions without specialty are not removed\")\n",
    "        \n",
    "    # 4- removing rows with NAN value for n_correct_options from our dataframe\n",
    "    if remove_nan_answer_type:\n",
    "        initial_shape=data.shape[0]\n",
    "        data = data.dropna(subset=[\"answer_type\"])\n",
    "        if verbose:\n",
    "            print(\"4- Removed {} samples with NA answer_type.\".format(initial_shape-data.shape[0]))\n",
    "            # write the same information in the verbose text file\n",
    "            with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "                text_file.write(\"4- Removed {} samples with NA answer_type.\\n\".format(initial_shape-data.shape[0]))\n",
    "        \n",
    "            \n",
    "    # 5- Add correct time/date columns\n",
    "    \n",
    "    original_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    desired_format = \"%Y%m%d%H%M%S\"\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "    data[\"timestamp\"] = data[\"timestamp\"].apply(lambda x: x.strftime(desired_format))\n",
    "\n",
    "    ## instead of the one above, we use the following code to get the correct time (if the time is in the format of integer)\n",
    "    # data[\"correct_time\"] = data[\"time\"].apply(lambda x:str(x).zfill(6))\n",
    "    # data[\"correct_date\"] = data[\"date\"].apply(lambda x:str(x))\n",
    "    # full_dates = [str1+str2 for str1, str2 in zip(data[\"correct_date\"],data[\"correct_time\"])]\n",
    "    # data[\"full_time\"] = full_dates\n",
    "    #data = data[[\"user_id\",\"item_id\",\"full_time\",\"correct\",\"group\"]]\n",
    "    \n",
    "    # add one second to each answer to order them correctly (bcs now the timestamp is the same for all answers in the same test)\n",
    "    # Convert timestamp to datetime format\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "    # Group by student and timestamp\n",
    "    groups = data.groupby(['student', 'timestamp'], sort=False,as_index=False)\n",
    "    # Sort each group by question_id\n",
    "    sorted_groups = groups.apply(lambda x: x.sort_values('question'))\n",
    "    # Assign a sequential count within each group\n",
    "    sorted_groups['answer_order'] = sorted_groups.groupby(['student', 'timestamp']).cumcount()\n",
    "    # Add the answer order as seconds to the timestamp\n",
    "    sorted_groups['timestamp'] += sorted_groups['answer_order'] * timedelta(seconds=1)\n",
    "    # Drop the answer_order column if no longer needed\n",
    "    sorted_groups.drop(columns=['answer_order'], inplace=True)\n",
    "    # Combine the sorted groups back into the original DataFrame order\n",
    "    data = sorted_groups.reset_index(drop=True)\n",
    "    \n",
    "    data.loc[:,\"timestamp\"] = data.loc[:,\"timestamp\"] - data.loc[:,\"timestamp\"].min()\n",
    "    data.loc[:,\"timestamp\"] = data.loc[:,\"timestamp\"].apply(lambda x: x.total_seconds()).astype(np.int64)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Converted timestamp to seconds.\")\n",
    "\n",
    "    # 6- binarize correct column\n",
    "    initial_shape = data.shape[0]\n",
    "    data.correct = data.correct.apply(lambda x:0 if x not in [0,1] else x).astype(np.int32)\n",
    "    data = data[data['correct'].isin([0,1])] # Remove potential continuous outcomes\n",
    "    if verbose:\n",
    "        print(\"6- Removed {} samples with non-binary outcomes.\".format(initial_shape-data.shape[0]))\n",
    "        # write the same information in the verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"6- Removed {} samples with non-binary outcomes.\\n\".format(initial_shape-data.shape[0]))\n",
    "    initial_shape = data.shape[0]\n",
    "    data['correct'] = data['correct'].astype(np.int32) # Cast outcome as int32\n",
    "    \n",
    "    \n",
    "    # 7- removing users without enough interaction (min_interactions_per_user)\n",
    "    # number of removed users (with less than min_interactions_per_user interactions) for the group 0 (train)\n",
    "    n_removed_users = data[data['group'] == 0].groupby(\"student\").filter(lambda x: len(x) < min_interactions_per_user).student.nunique()\n",
    "    # find the stundets with less than min_interactions_per_user interactions in the group 0 (train)\n",
    "    removed_users = data[data['group'] == 0].groupby(\"student\").filter(lambda x: len(x) < min_interactions_per_user).student.unique()\n",
    "    # remove the lines for the removed_users from the data\n",
    "    data = data[~data['student'].isin(removed_users)]\n",
    "    if verbose:\n",
    "        print('7- Removed {} samples that are data of {} unique users with less than {} interactions.'.format((initial_shape-data.shape[0]), n_removed_users, min_interactions_per_user))\n",
    "        print('percentage of removed unique users: {}%'.format(n_removed_users/data.student.nunique()*100))\n",
    "        # write the same information in the verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write('7- Removed {} samples that are data of {} unique users with less than {} interactions.\\n'.format((initial_shape-data.shape[0]), n_removed_users, min_interactions_per_user))\n",
    "            text_file.write('percentage of removed unique users: {}%.\\n'.format(n_removed_users/data.student.nunique()*100))\n",
    "            \n",
    "    # 8- removing questions without enough answers (min_answer_per_question)\n",
    "    # number of removed questions (with less than min_answer_per_question answers) for the group 0 (train)\n",
    "    n_removed_questions = data[data['group'] == 0].groupby(\"question\").filter(lambda x: len(x) < min_answer_per_question).question.nunique()\n",
    "    # find the questions with less than min_answer_per_question answers in the group 0 (train)\n",
    "    removed_questions = data[data['group'] == 0].groupby(\"question\").filter(lambda x: len(x) < min_answer_per_question).question.unique()\n",
    "    # remove the lines for the removed_questions from the data\n",
    "    data = data[~data['question'].isin(removed_questions)]\n",
    "    if verbose:\n",
    "        print('8- Removed {} samples that are data of {} unique questions with less than {} answers.'.format((initial_shape-data.shape[0]), n_removed_questions, min_answer_per_question))\n",
    "        print('percentage of removed unique questions: {}%'.format(n_removed_questions/data.question.nunique()*100))\n",
    "        # write the same information in the verbose text file\n",
    "        with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "            text_file.write('8- Removed {} samples that are data of {} unique questions with less than {} answers.\\n'.format((initial_shape-data.shape[0]), n_removed_questions, min_answer_per_question))\n",
    "            text_file.write('percentage of removed unique questions: {}%.\\n'.format(n_removed_questions/data.question.nunique()*100))\n",
    "        \n",
    "    # 9- Rename questions/skills\n",
    "    # Create variables\n",
    "    #data[\"item_id\"] = data[\"pb_id\"]\n",
    "    data = data[['student', 'question', 'n_options','answer_type','kc_id', 'correct', 'timestamp','group']]\n",
    "        \n",
    "    # Transform ids into numeric\n",
    "    data[\"user_id\"] = np.unique(data[\"student\"], return_inverse=True)[1].astype(np.int64)\n",
    "    data[\"item_id\"] = np.unique(data[\"question\"], return_inverse=True)[1].astype(np.int64)\n",
    "    \n",
    "    # Rename questions/skills in item_skills data\n",
    "    old_new_item_ids = data[~data.duplicated([\"question\",\"item_id\"])][[\"question\",\"item_id\"]]\n",
    "    old_new_user_ids = data[~data.duplicated([\"student\",\"user_id\"])][[\"student\",\"user_id\"]]\n",
    "    if not os.path.isdir(folder_name+'/'+ course_name+\"/processed\"):\n",
    "        os.makedirs(folder_name+'/'+ course_name+\"/processed\")\n",
    "    old_new_item_ids.to_csv(folder_name+'/'+ course_name+\"/processed/old_new_item_ids.csv\",index=False)\n",
    "    old_new_user_ids.to_csv(folder_name+'/'+ course_name+\"/processed/old_new_user_ids.csv\",index=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"9- Renamed questions and users and old-new ids saved.\")\n",
    "    # # call skills data\n",
    "    # spec_file= folder_name  +'/'+  course_name  +'/'+ 'questions_specialty.csv'\n",
    "    # item_skills = pd.read_csv(spec_file)\n",
    "    # item_skills = item_skills.merge(old_new_item_ids,on=\"question\",how=\"right\")\n",
    "    \n",
    "    # keep only the necessary columns\n",
    "    data = data[['user_id', 'item_id', 'n_options','answer_type' ,'timestamp','correct', 'kc_id', 'group']]\n",
    "    \n",
    "    # To be safe, drop duplicates again\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    if verbose:\n",
    "        if data.shape[0] < initial_shape:\n",
    "            logging.warning(\"{} duplicates have been found before saving the CSV.\".format(initial_shape-data.shape[0]))\n",
    "            print(\"10- {} duplicates have been found before saving the CSV.\".format(initial_shape-data.shape[0]))\n",
    "            # write the same information in the verbose text file\n",
    "            with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "                text_file.write(\"10- {} duplicates have been found before saving the CSV.\\n\".format(initial_shape-data.shape[0]))\n",
    "    initial_shape = data.shape[0]    \n",
    "    \n",
    "    # Create list of KCs\n",
    "    listOfKC = []\n",
    "    for kc_raw in data[\"kc_id\"].unique():\n",
    "        for elt in kc_raw.split('+'):\n",
    "            listOfKC.append(elt)\n",
    "    listOfKC = np.unique(listOfKC)\n",
    "\n",
    "    dict1_kc = {}\n",
    "    dict2_kc = {}\n",
    "    for k, v in enumerate(listOfKC):\n",
    "        dict1_kc[v] = k\n",
    "        dict2_kc[k] = v\n",
    "        \n",
    "        \n",
    "    # Build Q-matrix\n",
    "    Q_mat = np.zeros((len(data[\"item_id\"].unique()), len(listOfKC)))\n",
    "    item_skill = np.array(data[[\"item_id\",\"kc_id\"]])\n",
    "    for i in range(len(item_skill)):\n",
    "        splitted_kc = item_skill[i,1].split('+')\n",
    "        for kc in splitted_kc:\n",
    "            Q_mat[item_skill[i,0],dict1_kc[kc]] = 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"11- Computed q-matrix. Shape: {}.\".format(Q_mat.shape))\n",
    "        \n",
    "    \n",
    "    print(\"Data preprocessing done. Final output: {} samples.\".format((data.shape[0])))   \n",
    "    print(\"Number of unique students: {}.\".format(data.user_id.nunique()))\n",
    "    print(\"Number of unique questions: {}.\".format(data.item_id.nunique()))\n",
    "    print(\"Number of unique skills: {}.\".format(Q_mat.shape[1]))\n",
    "    print(\"Number of rows: {}.\".format(data.shape[0]))\n",
    "    \n",
    "    # write the same information in the verbose text file\n",
    "    with open(folder_name+'/'+ course_name+\"/processed/verbose.txt\", \"a\") as text_file:\n",
    "        text_file.write(\"Data preprocessing done. Final output: {} samples.\\n\".format((data.shape[0])))\n",
    "        text_file.write(\"Number of unique students: {}.\\n\".format(data.user_id.nunique()))\n",
    "        text_file.write(\"Number of unique questions: {}.\\n\".format(data.item_id.nunique()))\n",
    "        text_file.write(\"Number of unique skills: {}.\\n\".format(Q_mat.shape[1]))\n",
    "        text_file.write(\"Number of rows: {}.\\n\".format(data.shape[0]))\n",
    "\n",
    "    # Save preprocessed data\n",
    "    #data['timestamp'] =  pd.to_datetime(data['timestamp'])#, dayfirst=True)\n",
    "    data.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True)#first, timestamp should be converted to datetime\n",
    "    data.reset_index(inplace=True, drop=True) \n",
    "    sparse.save_npz(folder_name+'/'+ course_name+\"/processed/q_mat.npz\", sparse.csr_matrix(Q_mat))\n",
    "    data.to_csv(folder_name+'/'+ course_name+\"/processed/preprocessed_data.csv\", index=False)\n",
    "    \n",
    "    # split train and test and save them\n",
    "    train_set = data[data['group'] == 0]\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    #train_set['timestamp'] =  pd.to_datetime(train_set['timestamp'])#, dayfirst=True)\n",
    "    train_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True) #first, timestamp should be converted to datetime\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    test_set = data[data['group'] == 1]\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    #test_set['timestamp'] =  pd.to_datetime(test_set['timestamp'])#, dayfirst=True)\n",
    "    test_set.sort_values(by=[\"timestamp\", \"item_id\"], inplace=True)#first, timestamp should be converted to datetime\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    train_set.to_csv(folder_name+'/'+ course_name+\"/processed/train_set.csv\", encoding='utf-8', index = False)\n",
    "    test_set.to_csv(folder_name+'/'+ course_name+\"/processed/test_set.csv\", encoding='utf-8', index = False)\n",
    "    \n",
    "    # save skill_names_ids_map\n",
    "    # Convert the NumPy array to a pandas DataFrame and first column as specilaty column and  indices as  specilat_id column\n",
    "    skill_names_ids_map_df = pd.DataFrame(listOfKC,columns=['specialty'])\n",
    "    # have a specialty id column with values from 0 to n_skills\n",
    "    skill_names_ids_map_df['specialty_id'] = skill_names_ids_map_df.index\n",
    "    # Save the DataFrame as a CSV file\n",
    "    skill_names_ids_map_df.to_csv(folder_name+'/'+ course_name + '/processed/skill_names_ids_map.csv', index=False)\n",
    "\n",
    "    listOfKC = list(listOfKC)\n",
    "    # save listOfKC list\n",
    "    with open(folder_name+'/'+ course_name+'/processed/listOfKC.json', 'w') as fp:\n",
    "        json.dump(listOfKC, fp)\n",
    "    \n",
    "    # Save dict1_kc\n",
    "    with open(folder_name+'/'+ course_name+'/processed/dict_of_kc.json', 'w') as fp:\n",
    "        json.dump(dict1_kc, fp)\n",
    "        \n",
    "    # Save preprocessed data basic info\n",
    "    with open( folder_name+'/'+ course_name+\"/processed/config.json\", 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            'n_users': data.user_id.nunique(),\n",
    "            'n_items': data.item_id.nunique(),\n",
    "            'n_skills': Q_mat.shape[1]\n",
    "            }, indent=4))\n",
    "        \n",
    "\n",
    "    \n",
    "    return data, Q_mat, listOfKC, dict1_kc, train_set, test_set, skill_names_ids_map_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processed_data, q_mat, listOfKC, dict_of_kc, train_set, test_set = prepare_sides('data/kdd', 'sides_20_21', \\\n",
    " #                                                                  'SIDES_20_21_6thyear_withspec_training_prepared.csv', \\\n",
    "  #                                                                  'SIDES_20_21_6thyear_withspec_test_prepared.csv',\\\n",
    "   #                                                                 'specialty', 5, True, True, True)\n",
    " #\n",
    " \n",
    " #\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f029317bb4bfbc0fe13e73024792031a493f346c7d494ac5be7432f44fc5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
